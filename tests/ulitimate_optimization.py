import torch
import os
import time
import warnings
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.utils import logging

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")
logging.set_verbosity_error()


# ===================== ULTIMATE CPU OPTIMIZATION SETUP =====================

def setup_cpu_environment():
    """Configure optimal CPU environment for AMD EPYC 9334"""
    # AMD EPYC specific optimizations
    os.environ["OMP_NUM_THREADS"] = "32"  # Adjust based on your workload
    os.environ["MKL_NUM_THREADS"] = "32"
    os.environ["OPENBLAS_NUM_THREADS"] = "32"
    os.environ["VECLIB_MAXIMUM_THREADS"] = "32"
    os.environ["NUMEXPR_NUM_THREADS"] = "32"

    # Advanced CPU affinity settings for EPYC
    os.environ["KMP_AFFINITY"] = "granularity=fine,compact,1,0"
    os.environ["KMP_BLOCKTIME"] = "1"
    os.environ["KMP_SETTINGS"] = "1"

    # Memory allocation optimizations
    os.environ["MALLOC_CONF"] = "background_thread:true,metadata_thp:auto,dirty_decay_ms:30000,muzzy_decay_ms:30000"

    # PyTorch specific settings
    torch.set_num_threads(32)
    torch.set_num_interop_threads(4)
    torch.set_flush_denormal(True)

    # Enable all available CPU optimizations
    if hasattr(torch.backends, 'mkldnn') and torch.backends.mkldnn.is_available():
        torch.backends.mkldnn.enabled = True

    if hasattr(torch.backends, 'mkl') and torch.backends.mkl.is_available():
        torch.backends.mkl.enabled = True

    print("âœ“ CPU environment optimized for AMD EPYC 9334")


def load_optimized_model(model_name: str):
    """Load model with working optimizations only"""

    print("Loading model with available optimizations...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Load model with optimal settings
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,  # Use bfloat16 for AMD EPYC
        device_map="cpu",
        low_cpu_mem_usage=True,
        attn_implementation="eager",  # Better for CPU
        use_cache=True
    )

    # Apply model optimizations
    model = model.eval()
    model_type = "standard"

    # Try PyTorch 2.0 compilation (most effective optimization available)
    try:
        print("Attempting PyTorch compilation...")
        compiled_model = torch.compile(
            model,
            mode="reduce-overhead",  # Best for inference
            backend="inductor",
            dynamic=False,
            fullgraph=False  # More compatible
        )
        # Test the compiled model
        print("âœ“ PyTorch compilation successful")
        model = compiled_model
        model_type = "compiled"
    except Exception as e:
        print(f"âš  PyTorch compilation failed: {e}")
        print("Continuing with standard optimizations...")

    # Apply CPU-specific optimizations
    try:
        # Enable CPU optimizations
        torch.backends.cudnn.enabled = False  # Ensure CPU mode
        if hasattr(torch.backends, 'mkldnn'):
            torch.backends.mkldnn.enabled = True
        print("âœ“ CPU backend optimizations enabled")
    except Exception as e:
        print(f"âš  Backend optimization warning: {e}")

    return tokenizer, model, model_type


def create_optimized_generate_function(model, model_type):
    """Create optimized generation function with proper context"""

    def optimized_generate(inputs, **kwargs):
        """Highly optimized generation function"""

        # Prepare generation arguments for maximum speed
        generation_kwargs = {
            "max_new_tokens": 256,
            "do_sample": False,  # Greedy decoding is fastest
            "use_cache": True,  # Enable KV cache
            "temperature": None,
            "top_p": None,
            "top_k": None,
            "repetition_penalty": 1.0,
            "length_penalty": 1.0,
            "early_stopping": True,
            "num_beams": 1,  # Greedy search
            **kwargs  # Allow override
        }

        # Use the most efficient context manager
        context_manager = torch.inference_mode() if hasattr(torch, 'inference_mode') else torch.no_grad()

        with context_manager:
            outputs = model.generate(**inputs, **generation_kwargs)

        return outputs

    return optimized_generate


def benchmark_model(model, tokenizer, inputs, num_runs=3):
    """Comprehensive benchmarking"""

    print(f"\nðŸ”¥ Warming up model with {num_runs} runs...")

    # Create optimized generate function
    generate_func = create_optimized_generate_function(model, "optimized")
    input_ids_len = inputs["input_ids"].shape[-1]

    # Warmup runs
    warmup_times = []
    for i in range(num_runs):
        start_time = time.time()
        with torch.inference_mode():
            outputs = generate_func(inputs)
        warmup_time = time.time() - start_time
        warmup_times.append(warmup_time)
        print(f"Warmup {i + 1}: {warmup_time:.2f}s")

    # Get sample output for analysis
    generated_tokens = outputs[:, input_ids_len:]
    response = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)
    tokens_generated = len(generated_tokens[0])

    print(f"\nSample Response: {response[:100]}...")
    print(f"Tokens Generated: {tokens_generated}")

    # Performance benchmark runs
    print(f"\nðŸ“Š Running {num_runs} benchmark iterations...")
    benchmark_times = []

    for i in range(num_runs):
        start_time = time.time()
        with torch.inference_mode():
            outputs = generate_func(inputs)
        benchmark_time = time.time() - start_time
        benchmark_times.append(benchmark_time)
        print(f"Benchmark {i + 1}: {benchmark_time:.2f}s")

    return benchmark_times, tokens_generated, response


def main():
    print("ðŸš€ Starting Fixed Ultimate CPU Inference Optimization")
    print("=" * 60)

    # Setup environment
    setup_cpu_environment()

    # Model configuration
    model_name = "Salesforce/xLAM-2-1b-fc-r"

    print(f"\nðŸ“¦ Loading model: {model_name}")
    start_time = time.time()

    # Load optimized model
    tokenizer, model, model_type = load_optimized_model(model_name)

    load_time = time.time() - start_time
    print(f"âœ“ Model loaded in {load_time:.2f}s (Type: {model_type})")

    # Prepare test data
    messages = [
        {"role": "user", "content": "Hi, how are you?"},
        {"role": "assistant", "content": "Thanks. I am doing well. How can I help you?"},
        {"role": "user", "content": "What's the weather like in London?"},
    ]

    tools = [
        {
            "name": "get_weather",
            "description": "Get the current weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string", "description": "The city and state, e.g. San Francisco, CA"},
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"],
                             "description": "The unit of temperature to return"}
                },
                "required": ["location"]
            }
        }
    ]

    print("\nðŸ”„ Preparing inputs...")
    # Prepare inputs
    inputs = tokenizer.apply_chat_template(
        messages,
        tools=tools,
        add_generation_prompt=True,
        return_dict=True,
        return_tensors="pt"
    )

    # Add pad_token_id if not present
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    print("âœ“ Inputs prepared")

    # Run comprehensive benchmark
    benchmark_times, tokens_generated, sample_response = benchmark_model(
        model, tokenizer, inputs, num_runs=1
    )

    # Calculate statistics
    avg_time = sum(benchmark_times) / len(benchmark_times)
    min_time = min(benchmark_times)
    max_time = max(benchmark_times)

    avg_tokens_per_sec = tokens_generated / avg_time
    max_tokens_per_sec = tokens_generated / min_time

    print("\n" + "=" * 60)
    print("ðŸ“ˆ FINAL PERFORMANCE METRICS")
    print("=" * 60)
    print(f"Model Type: {model_type}")
    print(f"Model Load Time: {load_time:.2f}s")
    print(f"Average Inference Time: {avg_time:.2f}s")
    print(f"Best Inference Time: {min_time:.2f}s")
    print(f"Worst Inference Time: {max_time:.2f}s")
    print(f"Tokens Generated: {tokens_generated}")
    print(f"Average Tokens/Second: {avg_tokens_per_sec:.2f}")
    print(f"Peak Tokens/Second: {max_tokens_per_sec:.2f}")

    # Memory usage (if available)
    try:
        import psutil
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        print(f"Memory Usage: {memory_mb:.0f} MB")
    except ImportError:
        print("Memory usage: Install psutil for memory monitoring")

    print(f"\nSample Output: {sample_response}")

    # Performance improvement estimation
    original_time = 50.0  # Your original 50 second runtime
    improvement_factor = original_time / min_time
    print(f"\nðŸŽ¯ Estimated speedup: {improvement_factor:.1f}x faster than original")
    print(f"Time reduction: {original_time:.1f}s â†’ {min_time:.2f}s")

    # Optimization recommendations
    print("\nðŸ’¡ OPTIMIZATION RECOMMENDATIONS:")
    if model_type == "standard":
        print("- Install Intel Extension for PyTorch: pip install intel_extension_for_pytorch")
        print("- Try running with: numactl --cpunodebind=0 --membind=0 python script.py")

    print("- For maximum performance, consider model quantization")
    print("- Monitor CPU utilization during inference")

    print("\nâœ… Optimization Complete!")


if __name__ == "__main__":
    main()